import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from typing import TypedDict, Optional
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from tools.analysis import start_av_recording_tool, stop_av_recording_tool
from agents.question_agent import question_agent  # å·²æ”¯æŒå·¥å…·
from langgraph.prebuilt import ToolNode

# 1. å®šä¹‰çŠ¶æ€ç»“æ„
class AgentState(TypedDict):
    resume: str
    messages: list[BaseMessage]
    round: int
    audio_summary: Optional[str]
    video_summary: Optional[str]
    audio_dir: Optional[str]
    video_dir: Optional[str]
    should_continue: bool

# 2. ç»‘å®šå·¥å…·èŠ‚ç‚¹
start_tool_node = ToolNode([start_av_recording_tool])
stop_tool_node = ToolNode([stop_av_recording_tool])

# 3. æ¨¡æ‹Ÿå‰ç«¯æŒ‰é’®ç­‰å¾…ï¼ˆå®é™…å¯ç”¨è½®è¯¢äº‹ä»¶ï¼‰
def wait_for_user_click(state: AgentState):
    input("ğŸŸ¡ ç­‰å¾…ç”¨æˆ·ç­”å®Œé¢˜ï¼ŒæŒ‰ Enter åœæ­¢å½•åˆ¶...")
    return {}

# 4. å¤„ç†å·¥å…·è¿”å› observationï¼ˆæ¥è‡ª stop_av_recording_toolï¼‰
def extract_observation(state: AgentState, tool_output: dict) -> AgentState:
    return {
        **state,
        "audio_summary": tool_output.get("audio_summary", ""),
        "video_summary": tool_output.get("video_summary", ""),
        "audio_dir": tool_output.get("audio_dir"),
        "video_dir": tool_output.get("video_dir"),
    }

# 5. æé—®èŠ‚ç‚¹ï¼ˆåŠ ä¸Šåˆ†æè§‚å¯Ÿï¼‰
def question_agent_node(state: AgentState) -> AgentState:
    observation = (
        f"[éŸ³é¢‘åˆ†æ]ï¼š{state.get('audio_summary', '')}\n"
        f"[è§†é¢‘åˆ†æ]ï¼š{state.get('video_summary', '')}"
        if state.get("audio_summary") else ""
    )

    messages = state["messages"]
    if observation:
        messages.append(HumanMessage(content=observation))

    reply = question_agent.invoke(state)

    return {
        **state,
        "messages": messages + [reply["messages"][-1]],
        "round": state["round"] + 1,
        "should_continue": reply["should_continue"] and state["round"] < 9
    }

# 6. æ¡ä»¶è¾¹ï¼šæ˜¯å¦ç»§ç»­
def route_decision(state: AgentState) -> str:
    return "continue" if state["should_continue"] else "end"

# 7. æ„å»º LangGraph æµç¨‹
builder = StateGraph(AgentState)

builder.add_node("question", question_agent_node)
builder.add_node("start_tool", start_tool_node)
builder.add_node("wait_for_click", wait_for_user_click)
builder.add_node("stop_tool", stop_tool_node)
builder.add_node("update_observation", extract_observation)

builder.set_entry_point("question")
builder.add_edge("question", "start_tool")
builder.add_edge("start_tool", "wait_for_click")
builder.add_edge("wait_for_click", "stop_tool")
builder.add_edge("stop_tool", "update_observation")

builder.add_conditional_edges(
    "update_observation",
    route_decision,
    {
        "continue": "question",
        "end": END
    }
)

graph = builder.compile()

# 8. å¯åŠ¨æµ‹è¯•ç”¨åˆå§‹çŠ¶æ€ï¼ˆç¬¬ 0 è½®è‡ªæˆ‘ä»‹ç»ï¼‰
if __name__ == "__main__":
    init_state: AgentState = {
        "resume": "å§“åï¼šå¼ ä¸‰\nå­¦å†ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯æœ¬ç§‘\nç»å†ï¼šåœ¨è…¾è®¯å®ä¹ è¿‡ï¼Œåšè¿‡å¤§æ¨¡å‹å¾®è°ƒé¡¹ç›®ã€‚",
        "messages": [HumanMessage(content="è¯·åšä¸€æ¬¡è‡ªæˆ‘ä»‹ç»")],
        "round": 0,
        "audio_summary": "",
        "video_summary": "",
        "audio_dir": None,
        "video_dir": None,
        "should_continue": True
    }

    final_state = graph.invoke(init_state)
    print("âœ… é¢è¯•æµç¨‹å·²ç»“æŸ")
