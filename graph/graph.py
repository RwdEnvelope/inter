# graph/interview_flow_live.py
from __future__ import annotations
import sys, time
from pathlib import Path
from typing import TypedDict, List, Optional

# é¡¹ç›®æ ¹è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

# ==== ä¸šåŠ¡é€»è¾‘ ====
from agents.question_agent import question_agent
from tools.analysis   import start_av_recording, stop_av_recording
from langchain_core.messages import AIMessage, HumanMessage, AnyMessage

# ==== LangGraph ====
from langgraph.graph import StateGraph, START, END

# ------------------ çŠ¶æ€ ------------------
class InterviewState(TypedDict):
    resume: str
    round: int
    messages: List[AnyMessage]
    should_continue: bool

    # ç´¯ç§¯æ•°æ®
    qa_pairs:       List[tuple[str, str]]
    audio_summaries: List[str]
    video_summaries: List[str]

# ------------------ èŠ‚ç‚¹ ------------------
def ask(state: InterviewState) -> InterviewState:
    """é¢è¯•å®˜æé—®"""
    return question_agent(state)

def start_rec(state: InterviewState) -> InterviewState:
    """ä¸€æ—¦æé—®å°±å¼€å§‹å½•åˆ¶"""
    print("â–¶ï¸ æ­£åœ¨å½•åˆ¶éŸ³è§†é¢‘â€¦ï¼ˆå›ç­”å®ŒæŒ‰ Enter åœæ­¢ï¼‰")
    start_av_recording()
    return {}  # æ— éœ€å†™å›

def listen_and_stop(state: InterviewState) -> InterviewState:
    """å±•ç¤ºé—®é¢˜â†’æ”¶ç­”æ¡ˆâ†’åœæ­¢å½•åˆ¶å¹¶åˆ†æ"""
    # 1. æ‰“å°é¢è¯•å®˜é—®é¢˜
    ai_msg = next((m for m in state["messages"] if isinstance(m, AIMessage)), None)
    question_text = ai_msg.content if ai_msg else "(é—®é¢˜ä¸¢å¤±)"
    print(f"\né¢è¯•å®˜ï¼š{question_text}")

    # 2. å€™é€‰äººå›ç­”
    answer_text = input("å€™é€‰äººï¼š").strip()

    # 3. ç»“æŸå½•åˆ¶å¹¶æ‹¿åˆ†æç»“æœ
    result = stop_av_recording()
    print("âœ… å½•åˆ¶/åˆ†æå®Œæˆ\n")

    # 4. è¿”å›å¢é‡ï¼šHumanMessage + QA + æ‘˜è¦
    return {
        "messages": [HumanMessage(content=answer_text)],
        "qa_pairs": state.get("qa_pairs", []) + [(question_text, answer_text)],
        "audio_summaries": state.get("audio_summaries", []) + [result.get("audio_summary", "")],
        "video_summaries": state.get("video_summaries", []) + [result.get("video_summary", "")],
    }

def need_more(state: InterviewState) -> bool:
    """æ˜¯å¦ç»§ç»­ï¼Ÿ"""
    return state.get("should_continue", False) and state.get("round", 0) < 10

# ------------------ æ„å»º LangGraph ------------------
g = StateGraph(InterviewState)
g.add_node("ask",      ask)
g.add_node("startrec", start_rec)
g.add_node("listen",   listen_and_stop)

g.add_edge(START, "ask")
g.add_conditional_edges("ask", need_more, {True: "startrec", False: END})
g.add_edge("startrec", "listen")
g.add_edge("listen",   "ask")

interview_graph = g.compile()

# ------------------ CLI æ¼”ç¤º ------------------
if __name__ == "__main__":
    sample_resume = """å§“åï¼šAlice
å­¦å†ï¼šè®¡ç®—æœºç§‘å­¦æœ¬ç§‘
ç»å†ï¼šé˜¿é‡Œå·´å·´æ¨èç³»ç»Ÿå®ä¹  6 ä¸ªæœˆ
æŠ€èƒ½ï¼šPython / PyTorch"""

    init_state: InterviewState = {
        "resume": sample_resume,
        "round": 0,
        "messages": [],
        "should_continue": True,
        "qa_pairs": [],
        "audio_summaries": [],
        "video_summaries": [],
    }

    final = interview_graph.invoke(init_state)

    # ===== æ•´åœºç»“æœå±•ç¤º =====
    print("\n=== é¢è¯•ç»“æŸï¼Œå®Œæ•´é—®ç­”å›é¡¾ ===")
    for i, (q, a) in enumerate(final["qa_pairs"], 1):
        print(f"\nã€ç¬¬{i}é—®ã€‘{q}\nã€ç­”ã€‘{a}")

    print("\nğŸ“ å…¨åœºéŸ³é¢‘æ‘˜è¦ï¼š")
    print("\n".join(filter(None, final.get("audio_summaries", []))) or "æ— ")

    print("\nğŸï¸ å…¨åœºè§†é¢‘æ‘˜è¦ï¼š")
    print("\n".join(filter(None, final.get("video_summaries", []))) or "æ— ")
