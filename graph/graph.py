# graph/graph.py
from __future__ import annotations
import sys, time
from pathlib import Path
from typing import TypedDict, List, Optional, Annotated

# é¡¹ç›®æ ¹è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from tools.analysis import start_av_recording, stop_av_recording
from langchain_core.messages import AIMessage, HumanMessage, AnyMessage, ToolMessage

# ==== LangGraph ====
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode
from agents.question_agent import create_question_agent
from graph.av_workflow import av_interview_tool
from agents.analysis_agent import analysis_agent, InterviewAnalysisInput

# çŠ¶æ€å®šä¹‰
class InterviewState(TypedDict):
    resume: str
    round: int
    should_continue: bool
    messages: Annotated[List[AnyMessage], add_messages]
    qa_pairs: List[tuple[str, str]]
    audio_summaries: List[str]
    video_summaries: List[str]
    max_rounds: int
    # æ–°å¢å­—æ®µ
    interview_completed: bool
    analysis_result: dict
    structured_results: List[dict]

tools = [av_interview_tool]
tool_node = ToolNode(tools)

def assistant(state: InterviewState) -> InterviewState:
    """é¢è¯•å®˜åŠ©æ‰‹å‡½æ•° - æé—®å¹¶å†³å®šæ˜¯å¦ç»§ç»­"""
    current_round = state.get("round", 0)
    max_rounds = state.get("max_rounds", 10)
    
    # ç¬¬ä¸€è½®ï¼šè‡ªæˆ‘ä»‹ç»
    if current_round == 0:
        print(f"ğŸ¬ å¼€å§‹ç¬¬ {current_round + 1} è½®é¢è¯•")
        question = "è¯·å…ˆè‡ªæˆ‘ä»‹ç»ä¸€ä¸‹ã€‚"
        
        # âœ… å…³é”®ä¿®æ”¹ï¼šè¿”å›å·¥å…·è°ƒç”¨æ¶ˆæ¯è€Œä¸æ˜¯æ™®é€šæ¶ˆæ¯
        tool_call_message = AIMessage(
            content="",
            tool_calls=[{
                "name": "av_interview_tool", 
                "args": {"question": question, "max_duration": 30},
                "id": f"call_{current_round + 1}",
                "type": "tool_call"
            }]
        )
        
        return {
            "messages": [AIMessage(content=question), tool_call_message],
            "round": current_round + 1,
            "should_continue": True,
        }
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è½®æ¬¡
    if current_round >= max_rounds:
        print(f"â° å·²è¾¾åˆ°æœ€å¤§è½®æ¬¡ {max_rounds}ï¼Œç»“æŸé¢è¯•")
        return {
            "messages": [AIMessage(content="æ„Ÿè°¢æ‚¨çš„æ—¶é—´ï¼Œé¢è¯•ç»“æŸã€‚")],
            "round": current_round,
            "should_continue": False,
            "interview_completed": True,
        }
    
    # å…¶ä»–è½®æ¬¡ï¼šè°ƒç”¨LLMå†³ç­–
    try:
        print(f"ğŸ¬ å¼€å§‹ç¬¬ {current_round + 1} è½®é¢è¯•")
        llm = create_question_agent()
        decision = llm.invoke(state)
        
        print(f"ğŸ¤– å†³ç­–ç»“æœ: {decision}")
        
        messages = []
        should_continue = False
        
        # å¦‚æœæœ‰ä¸‹ä¸€ä¸ªé—®é¢˜ï¼Œåˆ™ç»§ç»­
        if decision.next_question:
            question = decision.next_question
            messages.append(AIMessage(content=question))
            
            # âœ… å…³é”®ä¿®æ”¹ï¼šç”Ÿæˆå·¥å…·è°ƒç”¨æ¶ˆæ¯
            tool_call_message = AIMessage(
                content="",
                tool_calls=[{
                    "name": "av_interview_tool", 
                    "args": {"question": question, "max_duration": 60},
                    "id": f"call_{current_round + 1}",
                    "type": "tool_call"
                }]
            )
            messages.append(tool_call_message)
            should_continue = decision.should_round
        else:
            # æ²¡æœ‰ä¸‹ä¸€ä¸ªé—®é¢˜ï¼Œç»“æŸé¢è¯•
            messages.append(AIMessage(content="æ„Ÿè°¢æ‚¨çš„æ—¶é—´ï¼Œé¢è¯•ç»“æŸã€‚"))
            should_continue = False
        
        return {
            "messages": messages,
            "round": current_round + 1,
            "should_continue": should_continue,



            
            "interview_completed": not should_continue,
        }
        
    except Exception as e:
        print(f"âŒ assistantå‡½æ•°é”™è¯¯: {e}")
        # å¼‚å¸¸æƒ…å†µä¸‹çš„å…œåº•å¤„ç†
        question = "è¯·ç»§ç»­ä»‹ç»æ‚¨çš„ç»éªŒã€‚"
        tool_call_message = AIMessage(
            content="",
            tool_calls=[{
                "name": "av_interview_tool", 
                "args": {"question": question, "max_duration": 60},
                "id": f"call_{current_round + 1}",
                "type": "tool_call"
            }]
        )
        return {
            "messages": [AIMessage(content=question), tool_call_message],
            "round": current_round + 1,
            "should_continue": True,
        }

def process_tool_results(state: InterviewState) -> InterviewState:
    """å¤„ç†å·¥å…·æ‰§è¡Œç»“æœ"""
    messages = state.get("messages", [])
    qa_pairs = list(state.get("qa_pairs", []))
    audio_summaries = list(state.get("audio_summaries", []))
    video_summaries = list(state.get("video_summaries", []))
    
    # æŸ¥æ‰¾æœ€æ–°çš„é—®é¢˜å’Œå·¥å…·ç»“æœ
    question = None
    tool_result = None
    
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content and not msg.tool_calls:
            question = msg.content
        elif isinstance(msg, ToolMessage):
            tool_result = msg.content
            break
    
    if question and tool_result:
        try:
            import json
            result_data = json.loads(tool_result)
            
            # æå–éŸ³é¢‘æ‘˜è¦å’Œè§†é¢‘æ‘˜è¦
            audio_summary = result_data.get("audio_summary", "")
            video_summary = result_data.get("video_summary", "")
            
            if audio_summary:
                audio_summaries.append(audio_summary)
            if video_summary:
                video_summaries.append(video_summary)
            
            # å°†é—®ç­”å¯¹åŠ å…¥è®°å½•
            qa_pairs.append((question, audio_summary))
            
            print(f"ğŸ“ æœ¬è½®é—®ç­”: Q: {question[:50]}... A: {audio_summary[:50]}...")
            
        except json.JSONDecodeError as e:
            print(f"âŒ è§£æå·¥å…·ç»“æœå¤±è´¥: {e}")
    
    return {
        "qa_pairs": qa_pairs,
        "audio_summaries": audio_summaries,
        "video_summaries": video_summaries,
    }

def analyze_interview_performance(state: InterviewState) -> InterviewState:
    """åˆ†æé¢è¯•è¡¨ç°çš„èŠ‚ç‚¹"""
    print("å¼€å§‹é¢è¯•åˆ†æ...")
    
    try:
        # å‡†å¤‡åˆ†æè¾“å…¥æ•°æ®
        analysis_input: InterviewAnalysisInput = {
            "resume": state.get("resume", ""),
            "qa_pairs": state.get("qa_pairs", []),
            "audio_summaries": state.get("audio_summaries", []),
            "video_summaries": state.get("video_summaries", []),
            "structured_results": state.get("structured_results", [])
        }
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = analysis_agent.analyze_interview(analysis_input)
        
        # æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå­—å…¸
        result_dict = {
            "overall_score": analysis_result.overall_score,
            "technical_competency": analysis_result.technical_competency,
            "communication_skills": analysis_result.communication_skills,
            "problem_solving": analysis_result.problem_solving,
            "strengths": analysis_result.strengths,
            "weaknesses": analysis_result.weaknesses,
            "recommendations": analysis_result.recommendations,
            "key_insights": analysis_result.key_insights,
            "behavioral_analysis": analysis_result.behavioral_analysis,
            "detailed_analysis": analysis_result.detailed_analysis
        }
        
        print("é¢è¯•åˆ†æå®Œæˆ")
        print(f"æ€»ä½“è¯„åˆ†: {analysis_result.overall_score}/100")
        print(f"ä¸»è¦ä¼˜åŠ¿: {', '.join(analysis_result.strengths[:2])}")
        
        return {
            "analysis_result": result_dict,
            "messages": [AIMessage(content=f"é¢è¯•åˆ†æå·²å®Œæˆã€‚å€™é€‰äººæ€»ä½“è¯„åˆ†ï¼š{analysis_result.overall_score}/100")]
        }
        
    except Exception as e:
        print(f"é¢è¯•åˆ†æå¤±è´¥: {e}")
        return {
            "analysis_result": {
                "error": f"åˆ†æå¤±è´¥: {str(e)}",
                "overall_score": 0,
                "status": "failed"
            },
            "messages": [AIMessage(content="é¢è¯•åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿè®¾ç½®ã€‚")]
        }

def route_after_tools(state: InterviewState) -> str:
    """å·¥å…·æ‰§è¡Œåçš„è·¯ç”±å†³ç­–"""
    should_continue = state.get("should_continue", False)
    interview_completed = state.get("interview_completed", False)
    
    if should_continue:
        route = "process_results"
    elif interview_completed:
        route = "analyze_performance"
    else:
        route = "__end__"
        
    print(f"å·¥å…·åè·¯ç”±: should_continue={should_continue}, completed={interview_completed} -> {route}")
    return route

def route_after_analysis(state: InterviewState) -> str:
    """é¢è¯•åˆ†æåçš„è·¯ç”±å†³ç­–"""
    print("åˆ†æå®Œæˆï¼Œç»“æŸæµç¨‹")
    return "__end__"

# âœ… æ„å»ºå›¾ - å¢å¼ºç‰ˆæœ¬
g = StateGraph(InterviewState)

# æ·»åŠ èŠ‚ç‚¹
g.add_node("assistant", assistant)
g.add_node("tools", tool_node)
g.add_node("process_results", process_tool_results)
g.add_node("analyze_performance", analyze_interview_performance)

# æ·»åŠ è¾¹
g.add_edge(START, "assistant")

# âœ… å…³é”®ä¿®æ”¹ï¼šassistantæœ‰å·¥å…·è°ƒç”¨æ—¶è‡ªåŠ¨å»tools
g.add_edge("assistant", "tools")

# âœ… toolsæ‰§è¡Œå®Œåçš„æ¡ä»¶è·¯ç”±
g.add_conditional_edges("tools", route_after_tools, {
    "process_results": "process_results",
    "analyze_performance": "analyze_performance",
    "__end__": END
})

# âœ… å¤„ç†å®Œç»“æœåå›åˆ°assistantç»§ç»­ä¸‹ä¸€è½®
g.add_edge("process_results", "assistant")

# âœ… åˆ†æå®Œæˆåç»“æŸ
g.add_edge("analyze_performance", END)

print("å›¾ç»“æ„éªŒè¯:")
print(f"èŠ‚ç‚¹: {list(g.nodes.keys())}")

memory = MemorySaver()
interview_graph = g.compile(checkpointer=memory)

print("å›¾ç¼–è¯‘æˆåŠŸ!")

if __name__ == "__main__":
    config = {"configurable": {"thread_id": "1"}}

    sample_resume = """å§“åï¼šAlice
å­¦å†ï¼šè®¡ç®—æœºç§‘å­¦æœ¬ç§‘
ç»å†ï¼šé˜¿é‡Œå·´å·´æ¨èç³»ç»Ÿå®ä¹  6 ä¸ªæœˆ
æŠ€èƒ½ï¼šPython / PyTorch"""

    init_state: InterviewState = {
        "resume": sample_resume,
        "round": 0,
        "max_rounds": 3,  # å‡å°‘è½®æ¬¡ä¾¿äºæµ‹è¯•
        "messages": [],
        "should_continue": True,
        "qa_pairs": [],
        "audio_summaries": [],
        "video_summaries": [],
        # æ–°å¢å­—æ®µ
        "interview_completed": False,
        "analysis_result": {},
        "structured_results": [],
    }

    print("ğŸ¯ å¼€å§‹æ™ºèƒ½é¢è¯•æµç¨‹...\n")
    print("ğŸ“‹ æ–°æµç¨‹: START â†’ assistant â†’ tools â†’ process_results â†’ assistant â†’ ... â†’ analyze_performance â†’ END")
    print("ğŸ” å¢åŠ åŠŸèƒ½: é¢è¯•åˆ†æ + å‘é‡æ•°æ®åº“ + ç½‘ç»œæœç´¢")
    print("=" * 80)
    
    final = None
    step_count = 0
    for chunk in interview_graph.stream(init_state, config=config, stream_mode="values"):
        final = chunk
        step_count += 1
        
        print(f"\nğŸ”„ æ­¥éª¤ {step_count}")
        
        if chunk.get("messages"):
            last_msg = chunk["messages"][-1]
            if isinstance(last_msg, AIMessage) and last_msg.content:
                print(f"ğŸ¤– é¢è¯•å®˜ [è½®æ¬¡{chunk.get('round', 0)}]: {last_msg.content}")
            elif isinstance(last_msg, AIMessage) and last_msg.tool_calls:
                print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {last_msg.tool_calls[0]['name']}")
            elif isinstance(last_msg, ToolMessage):
                print(f"ğŸ”§ å·¥å…·æ‰§è¡Œå®Œæˆ [è½®æ¬¡{chunk.get('round', 0)}]")
                result_preview = last_msg.content[:100] + "..." if len(last_msg.content) > 100 else last_msg.content
                print(f"ğŸ”§ å·¥å…·ç»“æœé¢„è§ˆ: {result_preview}")
        
        should_continue = chunk.get("should_continue", False)
        print(f"ğŸ“ ç»§ç»­é¢è¯•: {'æ˜¯' if should_continue else 'å¦'}")
        print("-" * 40)

    print("\n=== ğŸ‰ é¢è¯•ç»“æŸï¼Œå®Œæ•´å›é¡¾ ===")
    qa_pairs = final.get("qa_pairs", [])
    if qa_pairs:
        for i, (q, a) in enumerate(qa_pairs, 1):
            print(f"\nã€ç¬¬{i}é—®ã€‘{q}")
            print(f"ã€å›ç­”ã€‘{a}")
    else:
        print("æš‚æ— é—®ç­”è®°å½•")

    print(f"\nğŸ“ éŸ³é¢‘æ‘˜è¦: {len(final.get('audio_summaries', []))} æ¡")
    audio_summaries = final.get("audio_summaries", [])
    for i, summary in enumerate(audio_summaries, 1):
        if summary:
            print(f"  {i}. {summary[:100]}...")

    print(f"\nğŸï¸ è§†é¢‘æ‘˜è¦: {len(final.get('video_summaries', []))} æ¡")
    video_summaries = final.get("video_summaries", [])
    for i, summary in enumerate(video_summaries, 1):
        if summary:
            print(f"  {i}. {summary[:100]}...")
    
    # æ˜¾ç¤ºé¢è¯•åˆ†æç»“æœ
    analysis_result = final.get("analysis_result", {})
    if analysis_result and not analysis_result.get("error"):
        print(f"\nğŸ¯ === é¢è¯•åˆ†ææŠ¥å‘Š ===")
        print(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {analysis_result.get('overall_score', 0)}/100")
        print(f"ğŸ’» æŠ€æœ¯èƒ½åŠ›: {analysis_result.get('technical_competency', 0)}/100")
        print(f"ğŸ—£ï¸ æ²Ÿé€šèƒ½åŠ›: {analysis_result.get('communication_skills', 0)}/100")
        print(f"ğŸ§© é—®é¢˜è§£å†³: {analysis_result.get('problem_solving', 0)}/100")
        
        strengths = analysis_result.get("strengths", [])
        if strengths:
            print(f"\nâœ… ä¼˜åŠ¿: {', '.join(strengths)}")
            
        weaknesses = analysis_result.get("weaknesses", [])
        if weaknesses:
            print(f"âš ï¸ å¾…æ”¹è¿›: {', '.join(weaknesses)}")
            
        recommendations = analysis_result.get("recommendations", [])
        if recommendations:
            print(f"ğŸ’¡ å»ºè®®: {', '.join(recommendations)}")
            
        behavioral = analysis_result.get("behavioral_analysis", "")
        if behavioral:
            print(f"\nğŸ­ è¡Œä¸ºåˆ†æ: {behavioral}")
            
    elif analysis_result.get("error"):
        print(f"\nâŒ åˆ†æé‡åˆ°é—®é¢˜: {analysis_result.get('error')}")
    else:
        print(f"\nğŸ“ æœªç”Ÿæˆåˆ†ææŠ¥å‘Š")