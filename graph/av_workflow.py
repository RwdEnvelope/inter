# graph/av_workflow.py
from __future__ import annotations
import sys
import threading
import time
from pathlib import Path
from typing import TypedDict, Optional, Dict, Any, Annotated
import operator

# é¡¹ç›®æ ¹è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

from langgraph.graph import StateGraph, START, END
from langchain_core.tools import tool  # âœ… ä½¿ç”¨ @tool è£…é¥°å™¨
from langchain_core.messages import AnyMessage

# å‡è®¾è¿™äº›å‡½æ•°å·²å®ç°
from tools.analysis import start_av_recording, stop_av_recording

class AVState(TypedDict, total=False):
    """å­å›¾çš„è¿è¡ŒçŠ¶æ€"""
    messages: Annotated[list[Any], operator.add]
    recording_started: bool
    external_stop_signal: bool
    max_duration: int

def start_record(state: AVState) -> AVState:
    """å¯åŠ¨éŸ³è§†é¢‘å½•åˆ¶ï¼ˆéé˜»å¡ï¼‰"""
    start_av_recording()
    return {"recording_started": True}

def stop_record(state: AVState) -> AVState:
    """åœæ­¢å½•åˆ¶å¹¶æ”¶é›†ç»“æœ"""
    print("ğŸŸ¥ åœæ­¢å½•åˆ¶å¹¶æ•´ç†ç»“æœ")
    print("ğŸŸ¥ stop_record èŠ‚ç‚¹è¢«è§¦å‘")
    result = stop_av_recording()
    return {"messages": [result]}

def wait_for_stop(state: AVState) -> AVState:
    """ç­‰å¾…åœæ­¢ä¿¡å·"""
    print("ğŸŸ¡ å½•åˆ¶ä¸­ï¼ŒæŒ‰ Enter åœæ­¢ ...")
    t0 = time.time()
    max_duration = state.get("max_duration", 300)
    
    while True:
        # æ£€æŸ¥å¤–éƒ¨ä¿¡å·
        if state.get("external_stop_signal", False):
            print("ğŸ”´ å¤–éƒ¨åœæ­¢ä¿¡å·")
            return {}
        
        # æ£€æŸ¥é”®ç›˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import keyboard
            if keyboard.is_pressed("enter"):
                print("ğŸ”´ æŒ‰é”®åœæ­¢")
                return {"external_stop_signal": True}
        except:
            pass  # keyboard æ¨¡å—ä¸å¯ç”¨æ—¶å¿½ç•¥
        
        # æ£€æŸ¥è¶…æ—¶
        if time.time() - t0 > max_duration:
            print("â° è¶…æ—¶è‡ªåŠ¨åœæ­¢")
            return {"external_stop_signal": True}
            
        time.sleep(0.1)

# æ„å»ºå­å›¾
g = StateGraph(AVState)
g.add_node("start_record", start_record)
g.add_node("wait", wait_for_stop)
g.add_node("stop_record", stop_record)

g.add_edge(START, "start_record")
g.add_edge("start_record", "wait")
g.add_edge("wait", "stop_record")
g.add_edge("stop_record", END)

av_subgraph = g.compile()

# âœ… ä½¿ç”¨ @tool è£…é¥°å™¨é‡æ–°å®šä¹‰å·¥å…·
@tool
def av_interview_tool(question: str = "", max_duration: int = 300) -> str:
    """
    ä¸€æ¬¡æ€§å®Œæˆé¢è¯•çš„éŸ³è§†é¢‘é‡‡é›†ä¸åˆ†æï¼Œè¿”å›åˆ†æç»“æœ
    
    Args:
        question: é¢è¯•é—®é¢˜ï¼ˆå¯é€‰ï¼‰
        max_duration: æœ€å¤§å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300ç§’
    
    Returns:
        åŒ…å«éŸ³è§†é¢‘åˆ†æç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    print(f"ğŸ¬ å·¥å…·è¢«è°ƒç”¨: question='{question}', max_duration={max_duration}")
    
    # è®¾ç½®çŠ¶æ€
    state = {
        "max_duration": max_duration,
        "external_stop_signal": False,
        "messages": [],
        "recording_started": False
    }
    
    try:
        # æ‰§è¡Œå­å›¾
        result = av_subgraph.invoke(state)
        
        if 'messages' in result and result['messages']:
            actual_result = result['messages'][0]
            import json
            return json.dumps(actual_result, ensure_ascii=False, indent=2)
        else:
            return json.dumps({
                "audio_summary": "éŸ³è§†é¢‘å½•åˆ¶å®Œæˆï¼Œä½†æœªè·å¾—åˆ†æç»“æœ",
                "video_summary": "",
                "audio_dir": "",
                "video_dir": ""
            }, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ å½•åˆ¶å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
        import json
        return json.dumps({
            "error": f"å½•åˆ¶å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
            "audio_summary": "",
            "video_summary": "",
            "audio_dir": "",
            "video_dir": ""
        }, ensure_ascii=False, indent=2)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•å·¥å…·...")
    
    # æ¨¡æ‹Ÿè‡ªåŠ¨åœæ­¢
    shared_state = {"max_duration": 10}
    
    def stop_later(sec: int = 5):
        time.sleep(sec)
        shared_state["external_stop_signal"] = True
        print(f"â›”ï¸ {sec} ç§’åˆ°ï¼Œè‡ªåŠ¨å‘é€ stop ä¿¡å·")

    threading.Thread(target=stop_later, daemon=True).start()
    
    # æµ‹è¯•å·¥å…·è°ƒç”¨
    res = av_interview_tool.invoke({"question": "è¯·è‡ªæˆ‘ä»‹ç»", "max_duration": 10})
    print("âœ… å®Œæˆï¼Œè¿”å›ç»“æœï¼š")
    print(res)